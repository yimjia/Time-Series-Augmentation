{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation using the Trained Convolutional Variational Autoencoder (CVAE)\n",
    "\n",
    "### Yiming Jia</br> 2024-09-18\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined parameters\n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File directory (change the directory to the folder including your time series data)\n",
    "file_dir = 'C:/Users/jiayi/Desktop/CVAE Codes for GitHub/'\n",
    "\n",
    "\n",
    "# Latent space size (usually it is a small number e.g., <= 8)\n",
    "latent_dim = 4\n",
    "\n",
    "\n",
    "# Number of epochs\n",
    "NUM_EPOCH = 50000\n",
    "\n",
    "\n",
    "# Stop training when loss reaching\n",
    "Loss_Stop = 0.0000\n",
    "\n",
    "\n",
    "# Initial learning rate for Adam optimizer\n",
    "Learning_Rate = 0.005\n",
    "\n",
    "\n",
    "# Initial learning rate step decay parameters\n",
    "Drop_Rate1 = 0.80\n",
    "NUM_EPOCH_DROP1 = 5000\n",
    "\n",
    "Drop_Rate2 = 0.90\n",
    "NUM_EPOCH_DROP2 = 10000\n",
    "\n",
    "Drop_Rate3 = 0.95\n",
    "NUM_EPOCH_DROP3 = 20000\n",
    "\n",
    "Drop_Rate4 = 1.00\n",
    "\n",
    "\n",
    "# Drop learning rate every Drop_Epoch\n",
    "Drop_Epoch = 500\n",
    "\n",
    "\n",
    "# Define a GPU usage\n",
    "# CPU = -1, GPU0 = 0, GPU1 = 1\n",
    "GPU_ID = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Load packages and data\n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the GPU is specified\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID) \n",
    "\n",
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for model, data, and figure\n",
    "import shutil\n",
    "\n",
    "# Folder name\n",
    "ID = f\"_Lat{latent_dim}_Epoch{NUM_EPOCH}_Aug\" \n",
    "\n",
    "# Model\n",
    "model_dir = f\"Model{str(ID)}\" \n",
    "modelExist = os.path.exists(model_dir)\n",
    "if modelExist:\n",
    "    shutil.rmtree(model_dir)\n",
    "    os.makedirs(model_dir) \n",
    "    print('Model folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(model_dir)    \n",
    "    print('Model folder does not exist, create this folder.')\n",
    "    \n",
    "# Data\n",
    "data_dir = f\"Data{str(ID)}\" \n",
    "dataExist = os.path.exists(data_dir)\n",
    "if dataExist:\n",
    "    shutil.rmtree(data_dir)\n",
    "    os.makedirs(data_dir) \n",
    "    print('Data folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(data_dir)    \n",
    "    print('Data folder does not exist, create this folder.')\n",
    "        \n",
    "# Figure\n",
    "figure_dir = f\"Figure{str(ID)}\" \n",
    "figureExist = os.path.exists(figure_dir)\n",
    "if figureExist:\n",
    "    shutil.rmtree(figure_dir)    \n",
    "    os.makedirs(figure_dir) \n",
    "    print('Figure folder exists, delete the current files in this folder.')\n",
    "else:\n",
    "    os.makedirs(figure_dir)    \n",
    "    print('Figure folder does not exist, create this folder.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "from tensorflow.keras import models, layers\n",
    "from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, Flatten, Reshape, Conv2DTranspose, Lambda, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import mse\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import time\n",
    "from keras.layers import ReLU\n",
    "from keras.layers import PReLU\n",
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the time series data\n",
    "Data = scipy.io.loadmat(file_dir + 'TimeSeries.mat')\n",
    "# Notes: \n",
    "# 'TimeSeries.mat' is a Nt*1 vector of cells (Nt is the number of time series).\n",
    "# Each cell includes a Ns*D matrix (Ni is the number of time steps, D is the number of dimensions).\n",
    "\n",
    "T_Raw = Data[\"TimeSeries\"]     \n",
    "\n",
    "# Reshape the time series data\n",
    "T_Raw = np.squeeze(T_Raw)\n",
    "T = np.empty((T_Raw.shape[0], T_Raw[0].shape[0], T_Raw[0].shape[1]))\n",
    "for i in range(T_Raw.shape[0]):\n",
    "    T[i] = T_Raw[i]\n",
    "\n",
    "T = T.reshape(-1, T.shape[1],T.shape[2], 1)\n",
    "\n",
    "print('')\n",
    "print('Time series data shape = ', T.shape)\n",
    "print('')\n",
    "print('Number of time series =', T.shape[0])\n",
    "print('Number of time steps in each time series =', T.shape[1])\n",
    "print('Number of dimensions in each time series = ', T.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Encoder\n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_encoder = Input(shape=(T.shape[1],T.shape[2],T.shape[3]), name = 'x_T')\n",
    "\n",
    "\n",
    "# Activation function\n",
    "act_fun = LeakyReLU(alpha=0.5)\n",
    "# act_fun = 'tanh'\n",
    "# act_fun = 'ReLU'\n",
    "# act_fun = 'PReLU'\n",
    "\n",
    "\n",
    "# Convolutional and max pooling layers\n",
    "x_C1 = Conv2D(filters = 16, kernel_size = (3,3), padding=\"same\",\n",
    "              activation=act_fun, name = 'x_C1',\n",
    "              kernel_initializer=initializers.GlorotNormal,\n",
    "              bias_initializer=initializers.Zeros())(input_encoder)\n",
    "\n",
    "x_P1 = MaxPooling2D(pool_size = (3,1), strides = (2,1), padding='same', name = 'x_P1')(x_C1)\n",
    "\n",
    "x_C2 = Conv2D(filters = 8, kernel_size = (3,3), padding=\"same\", \n",
    "              activation=act_fun, name = 'x_C2',\n",
    "              kernel_initializer=initializers.GlorotNormal,\n",
    "              bias_initializer=initializers.Zeros())(x_P1)\n",
    "\n",
    "x_P2 = MaxPooling2D(pool_size = (3,1), strides = (2,1), padding='same', name = 'x_P2')(x_C2)\n",
    "\n",
    "x_C3 = Conv2D(filters = 4, kernel_size = (3,3), padding=\"same\",\n",
    "              activation=act_fun, name = 'x_C3',\n",
    "              kernel_initializer=initializers.GlorotNormal,\n",
    "              bias_initializer=initializers.Zeros())(x_P2)\n",
    "\n",
    "\n",
    "# Flatten layer\n",
    "x_F = Flatten(name = 'x_F')(x_C3)\n",
    "\n",
    "\n",
    "# Dense layers\n",
    "x_D1 = Dense(512, activation=act_fun, name = 'x_D1',\n",
    "                  kernel_initializer=initializers.GlorotNormal,\n",
    "                  bias_initializer=initializers.Zeros())(x_F)\n",
    "x_D2 = Dense(128, activation=act_fun, name = 'x_D2',\n",
    "                  kernel_initializer=initializers.GlorotNormal,\n",
    "                  bias_initializer=initializers.Zeros())(x_D1)\n",
    "x_D3 = Dense(32, activation=act_fun, name = 'x_D3',\n",
    "                  kernel_initializer=initializers.GlorotNormal,\n",
    "                  bias_initializer=initializers.Zeros())(x_D2)\n",
    "x_D4 = Dense(8, activation=act_fun, name = 'x_D4',\n",
    "                 kernel_initializer=initializers.GlorotNormal,\n",
    "                 bias_initializer=initializers.Zeros())(x_D3)\n",
    "\n",
    "\n",
    "# Latent feature layer\n",
    "# Mean\n",
    "encoder_mu = Dense(latent_dim, name=\"Encoder_Mu\")(x_D4)\n",
    "# Log-variance\n",
    "encoder_log_variance = Dense(latent_dim, name=\"Encoder_LogVariance\")(x_D4)\n",
    "\n",
    "\n",
    "# Sampling\n",
    "def sampling(mu_log_variance):\n",
    "    mu, log_variance = mu_log_variance\n",
    "    epsilon = tensorflow.keras.backend.random_normal(shape=tensorflow.keras.backend.shape(mu), mean=0.0, stddev=1.0)\n",
    "    random_sample = mu + tensorflow.keras.backend.exp(log_variance)*epsilon\n",
    "    return random_sample\n",
    "\n",
    "\n",
    "# Encoder\n",
    "output_encoder = Lambda(sampling, name=\"Encoder_Output\")([encoder_mu, encoder_log_variance])\n",
    "encoder = Model([input_encoder], [output_encoder], name='encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Decoder \n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer\n",
    "input_decoder = Input(shape=(latent_dim), name = 'Decoder_Input')\n",
    "\n",
    "\n",
    "# Denses layer\n",
    "y_D1 = Dense(8, activation=act_fun, name = 'y_D1',\n",
    "                kernel_initializer=initializers.GlorotNormal,\n",
    "                bias_initializer=initializers.Zeros())(input_decoder)\n",
    "y_D2 = Dense(32, activation=act_fun, name = 'y_D2',\n",
    "                 kernel_initializer=initializers.GlorotNormal,\n",
    "                 bias_initializer=initializers.Zeros())(y_D1)\n",
    "y_D3 = Dense(128, activation=act_fun, name = 'y_D3',\n",
    "                  kernel_initializer=initializers.GlorotNormal,\n",
    "                  bias_initializer=initializers.Zeros())(y_D2)\n",
    "y_D4 = Dense(512, activation=act_fun, name = 'y_D4',\n",
    "                  kernel_initializer=initializers.GlorotNormal,\n",
    "                  bias_initializer=initializers.Zeros())(y_D3)\n",
    "y_D5 = Dense(x_F.shape[1], activation=act_fun, name = 'y_D5',\n",
    "                           kernel_initializer=initializers.GlorotNormal,\n",
    "                           bias_initializer=initializers.Zeros())(y_D4)\n",
    "\n",
    "\n",
    "# Inverse flatten layer\n",
    "y_F = Reshape([x_C3.shape[1], x_C3.shape[2],x_C3.shape[3]], name = 'y_F')(y_D5)\n",
    "\n",
    "\n",
    "# Deconvolutional layers\n",
    "y_C1 = Conv2DTranspose(filters = 8, strides = (1,1), kernel_size = (3,3), padding=\"same\", \n",
    "                       activation=act_fun, name = 'y_C1',\n",
    "                       kernel_initializer=initializers.GlorotNormal,\n",
    "                       bias_initializer=initializers.Zeros())(y_F)                                                     \n",
    "\n",
    "y_C2 = Conv2DTranspose(filters = 8, strides = (2,1), kernel_size = (3,3), padding=\"same\", \n",
    "                       activation=act_fun, name = 'y_C2',\n",
    "                       kernel_initializer=initializers.GlorotNormal,\n",
    "                       bias_initializer=initializers.Zeros())(y_C1)                                                     \n",
    "\n",
    "y_C3 = Conv2DTranspose(filters = 16, strides = (1,1), kernel_size = (3,3), padding=\"same\", \n",
    "                       activation=act_fun, name = 'y_C3',\n",
    "                       kernel_initializer=initializers.GlorotNormal,\n",
    "                       bias_initializer=initializers.Zeros())(y_C2)  \n",
    "\n",
    "y_C4 = Conv2DTranspose(filters = 16, strides = (2,1), kernel_size = (3,3), padding=\"same\", \n",
    "                       activation=act_fun, name = 'y_C4',\n",
    "                       kernel_initializer=initializers.GlorotNormal,\n",
    "                       bias_initializer=initializers.Zeros())(y_C3)   \n",
    "\n",
    "y_C5 = Conv2DTranspose(filters = 1, kernel_size = (3,3), padding=\"same\", \n",
    "                       activation=act_fun, name = 'y_C5',\n",
    "                       kernel_initializer=initializers.GlorotNormal,\n",
    "                       bias_initializer=initializers.Zeros())(y_C4)    \n",
    "\n",
    "output_decoder = Conv2DTranspose(filters = 1, kernel_size = (3,3), padding=\"same\", \n",
    "                                 activation='linear', name = 'Decorder_Output',\n",
    "                                 kernel_initializer=initializers.GlorotNormal,\n",
    "                                 bias_initializer=initializers.Zeros())(y_C5) \n",
    "\n",
    "\n",
    "# Decoder\n",
    "decoder = Model([input_decoder], [output_decoder], name='decoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Autoencoder \n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input\n",
    "input_auto = input_encoder\n",
    "\n",
    "# Output\n",
    "output_auto = decoder(encoder(input_encoder))\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = Model([input_auto], [output_auto], name='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Training parameters\n",
    "### (same as P1_CVAE.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint ID\n",
    "ID_CVAE = f\"_Lat{latent_dim}_Epoch{NUM_EPOCH}\" \n",
    "model_auto_dir = f\"Model{str(ID_CVAE)}\" \n",
    "Checkpoint_ID = os.path.join(model_auto_dir, \"Checkpoint.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Load the trained autoencoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "autoencoder_best = autoencoder\n",
    "\n",
    "# Load weights\n",
    "autoencoder_best.load_weights(Checkpoint_ID);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "decoder_best = decoder\n",
    "\n",
    "# Load weights from autoencoder_best\n",
    "encoder_len = len(encoder.weights)\n",
    "decoder_len = len(decoder_best.weights)\n",
    "decoder_best.set_weights(autoencoder_best.weights[encoder_len:encoder_len+decoder_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "## Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "# Load the latent features generated for augmentation\n",
    "Aug = scipy.io.loadmat(file_dir + 'Aug_Lat.mat')\n",
    "    \n",
    "Random = Aug[\"Lat_Ran\"]     \n",
    "\n",
    "print('Encoder augmented data shape:', Random.shape)\n",
    "print('')\n",
    "    \n",
    "    \n",
    "# Generate the augmentated data using the decorder\n",
    "decoder_aug_raw = decoder_best.predict(Random,verbose = 0);\n",
    "decoder_aug = decoder_aug_raw.reshape(Random.shape[0],T.shape[1]*T.shape[2]);\n",
    "np.savetxt(data_dir + '/Aug.dat', decoder_aug);\n",
    "    \n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The running time:\", round((end-start)/60,1), \"mins\")\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
